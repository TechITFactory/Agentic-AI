# Classification Metrics: Precision, Recall, F1

## Why this lesson

Pick the right metric for the problem instead of defaulting to accuracy.

## Outcomes

- ✅ Compute precision/recall/F1, ROC-AUC, PR-AUC
- ✅ Choose metrics based on business cost
- ✅ Read confusion matrices

**Time**: ~25 minutes

**Prerequisites**: Splits lesson

## Agenda

- Confusion matrix basics
- Precision/recall/F1, ROC vs PR curves
- Metric selection by use case (fraud, churn, support)

## Hands-on

- Compute metrics on a sample classifier
- Plot ROC and PR curves

## Deliverable

- Mini report picking a primary metric for a given scenario

## Checkpoint

Can you justify why PR-AUC matters more than ROC-AUC for rare positive rates?
