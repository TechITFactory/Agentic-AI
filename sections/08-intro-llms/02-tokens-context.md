# Tokens and Context Windows

## Why this lesson

Manage prompt length to avoid truncation and blown budgets.

## Outcomes

- ✅ Measure token lengths of prompts
- ✅ Plan prompts to fit within model context
- ✅ Understand truncation vs error behaviors

**Time**: ~20 minutes

**Prerequisites**: LLM fundamentals

## Agenda

- Tokenization tools
- Context window limits across models
- Truncation strategies

## Hands-on

- Use a tokenizer to check prompt sizes
- Create a truncated vs condensed version; compare

## Deliverable

- Table of token counts for original vs condensed prompts

## Checkpoint

Can you guarantee your prompt fits a 16k context model?
