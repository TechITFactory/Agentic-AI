# LLM Fundamentals

## Why this lesson
Ground yourself in what LLMs are and are not before using them.

## Outcomes
- ✅ Explain tokens, embeddings, and autoregressive generation
- ✅ Describe capabilities vs limitations
- ✅ Know latency/cost implications of context length

**Time**: ~20 minutes

**Prerequisites**: Python comfort

## Agenda
- How LLMs generate text
- Context window and tokenization
- Latency/cost basics

## Hands-on
- Tokenize a few sample prompts; count tokens
- Estimate cost for a sample request

## Deliverable
- Short note mapping prompt → token count → estimated cost

## Checkpoint
Can you explain why longer prompts increase latency and cost?