# Training and Evaluation

## Why this module

Train candidate models, compare against baselines, and pick a champion with evidence.

## Outcomes

- ✅ Reproducible training script/config
- ✅ Baseline vs improved models with metrics
- ✅ Champion selection rationale

**Time**: ~60-90 minutes

**Tasks
1) Prepare train/val/test splits
2) Train baseline (logistic regression) and a stronger model (forest/XGBoost)
3) Record metrics (F1, PR-AUC) and confusion matrices
4) Choose champion and log params/seed

**Deliverable
- Training notebook/script, metrics table, and saved artifacts

**Checkpoint
Can you justify the champion model choice with metrics and reproducibility details?
